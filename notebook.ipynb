{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/tanulsingh077/deep-learning-for-nlp-zero-to-transformers-bert/notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%bash\n",
    "## Install miniconda\n",
    "# mkdir -p $HOME/miniconda3\n",
    "# wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O $HOME/miniconda3/miniconda.sh\n",
    "# bash $HOME/miniconda3/miniconda.sh -b -u -p $HOME/miniconda3\n",
    "\n",
    "# $HOME/miniconda3/bin/conda init bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%bash\n",
    "# # Set up conda environment and install dependencies from requirements.txt\n",
    "# conda create -y \\\n",
    "#   --name dl-for-nlp \\\n",
    "#   --channel conda-forge \\\n",
    "#   --file conda-requirements.txt\n",
    "\n",
    "# conda activate dl-for-nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%bash\n",
    "## Set up GPU\n",
    "## https://www.tensorflow.org/install/pip#linux\n",
    "\n",
    "# conda install -y cudatoolkit=11.2 cudnn=8.1.0 \n",
    "# export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CONDA_PREFIX/lib/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%bash\n",
    "# # Install Python dependencies\n",
    "# conda install -y jupyter \\\n",
    "#   kaggle \\\n",
    "#   numpy \\\n",
    "#   pandas \\\n",
    "#   plotly \\\n",
    "#   python-dotenv \\\n",
    "#   scikit-learn \\\n",
    "#   tensorflow \\\n",
    "#   tqdm\n",
    "\n",
    "# conda list -e > conda-requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import os\n",
    "import pandas\n",
    "import plotly\n",
    "import tensorflow as tf\n",
    "from dotenv import load_dotenv\n",
    "from sklearn import model_selection\n",
    "from tqdm import tqdm\n",
    "\n",
    "envfile = '.env'\n",
    "load_dotenv(envfile)\n",
    "\n",
    "input_dir = os.getenv('INPUT_DIR', '.')\n",
    "output_dir = os.getenv('OUTPUT_DIR', '.')\n",
    "\n",
    "data_sample_size = int(os.getenv('DATA_SAMPLE_SIZE', 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "  print('Running on TPU ', tpu.master())\n",
    "except ValueError:\n",
    "  tpu = None\n",
    "\n",
    "if tpu:\n",
    "  tf.config.experimental_connect_to_cluster(tpu)\n",
    "  tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "  strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "else:\n",
    "  strategy = tf.distribute.get_strategy()\n",
    "\n",
    "print('Replicas: ', strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pandas.read_csv(f'{input_dir}/jigsaw-toxic-comment-train.csv')\n",
    "valid = pandas.read_csv(f'{input_dir}/validation.csv')\n",
    "test = pandas.read_csv(f'{input_dir}/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_columns = [\n",
    "  'identity_hate',\n",
    "  'insult',\n",
    "  'obscene',\n",
    "  'severe_toxic',\n",
    "  'threat'\n",
    "]\n",
    "\n",
    "train = train.drop(drop_columns, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contains_dropped_columns = train.columns.isin(drop_columns).any()\n",
    "assert contains_dropped_columns == False, 'Dataframe contains dropped columns'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.sample(n = data_sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, xvalid, ytrain, yvalid = model_selection.train_test_split(train.comment_text.values, train.toxic.values, \n",
    "                                    stratify = train.toxic.values, random_state = 2023, test_size = 0.2, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_max_length = train.comment_text.str.split(' ').agg(len).max()\n",
    "\n",
    "text_vectorizer = tf.keras.layers.TextVectorization(\n",
    "  max_tokens = None,\n",
    "  standardize = 'lower_and_strip_punctuation',\n",
    "  split = 'whitespace',\n",
    "  ngrams = None,\n",
    "  output_mode = 'int',  # todo: compare with tf-idf\n",
    "  output_sequence_length = int(comment_max_length),\n",
    "  pad_to_max_tokens = False,\n",
    "  vocabulary = None,\n",
    "  idf_weights = None,\n",
    "  sparse = False,\n",
    "  ragged = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorizer.adapt(numpy.concatenate([xtrain, xvalid]))\n",
    "\n",
    "vocab_size = len(text_vectorizer.get_vocabulary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "  model = tf.keras.Sequential()\n",
    "  model.add(tf.keras.layers.Embedding(vocab_size, 300, input_length = comment_max_length))\n",
    "  model.add(tf.keras.layers.SimpleRNN(100))\n",
    "  model.add(tf.keras.layers.Dense(1, activation = 'sigmoid'))\n",
    "  model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = [tf.keras.metrics.Accuracy()])\n",
    "  model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-for-nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "25bca81d881cb77cd260f7791f716681ad96003992050e02b810c0d18fa0e4c8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
